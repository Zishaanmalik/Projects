{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45836f69-5fd4-4e68-bef7-45576f37d16c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dd99ab-4ec1-4eab-81f5-9351333a2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bb069-4d56-4cc3-9f75-7049c85490f1",
   "metadata": {},
   "source": [
    "## Using device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66091d94-746f-4744-91e2-c8a51c87d087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55824965-1af8-417c-9da9-db3313d8552d",
   "metadata": {},
   "source": [
    "## Load Custom CSV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f536a530-684b-453b-b4c2-0fb704fdb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root=Path(\"data\")\n",
    "root.mkdir(exist_ok=True)\n",
    "path= root / \"hindi_english_parallel.csv\"\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcca5990-6598-49ee-ba63-c0f1f114ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.sample(5050,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dc962a-83b7-4176-b975-b8f69c6810b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0882c0bd-b8b5-4514-b6b2-4e8991426d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7cbe3b-963c-446b-9291-886174b3fd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5007"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0fe30e-e469-4794-862b-624ab83f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_texts = df.iloc[:, 1].astype(str).str.lower().tolist()\n",
    "tgt_texts = df.iloc[:, 0].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742473e0-9b02-44a3-a21f-03f88f797fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['बडे पैमाने पर सुनामी से प्रभावीत जापान में 4 दिनो बाद कोई अभी तक जिंदा होने की आशाएँ लुप्त हो रही थी। ',\n",
       " 'वर्ग का पूर्णा क्या था? ',\n",
       " 'मैं अपना काम कर चुका हूँ। ',\n",
       " 'राष्ट्रीय मनः स्वास्थ्य कार्यक्रम',\n",
       " 'क्रियावली',\n",
       " 'मुद्रास्फीति की दर प्रत्यक्ष रूप से उपरली डिग्री में है। ',\n",
       " 'वाहिका में रूधिर अवपंक रक्त प्रवाह में अवरोध का कारण है',\n",
       " 'उपकरण',\n",
       " 'हरकत-उल-जिहाद-ए-इस्लामी',\n",
       " 'URL …']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f085a1f-f59a-4d8f-944d-85cc6b981b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4 days after the massive tsunami struck japan, hopes of finding anyone still alive were fading.',\n",
       " 'what was completing the square?',\n",
       " 'i have already done my work.',\n",
       " 'national mental health programme',\n",
       " 'menu',\n",
       " 'the inflation rate is apparently in the ascending degree.',\n",
       " 'a sludge of blood in the vessel causes absruction to blood flow.',\n",
       " 'device',\n",
       " 'harkat - ul - jihad al - islami',\n",
       " 'url …']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf96e84-fb30-454c-b35c-65d9bd1dc523",
   "metadata": {},
   "source": [
    "## Add `<SOS>` and `<EOS>` to TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf744c4-ea54-4c47-8512-646750307971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens\n",
    "src_texts = [f\"<sos> {text} <eos>\" for text in src_texts]\n",
    "tgt_texts = [f\"<sos> {text} <eos>\" for text in tgt_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0857d6-f974-4a27-9cd7-644cd62eda9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> 4 days after the massive tsunami struck japan, hopes of finding anyone still alive were fading. <eos>',\n",
       " '<sos> what was completing the square? <eos>',\n",
       " '<sos> i have already done my work. <eos>',\n",
       " '<sos> national mental health programme <eos>',\n",
       " '<sos> menu <eos>',\n",
       " '<sos> the inflation rate is apparently in the ascending degree. <eos>',\n",
       " '<sos> a sludge of blood in the vessel causes absruction to blood flow. <eos>',\n",
       " '<sos> device <eos>',\n",
       " '<sos> harkat - ul - jihad al - islami <eos>',\n",
       " '<sos> url … <eos>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f576b3d-5e43-4c61-967c-982b7495fe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> बडे पैमाने पर सुनामी से प्रभावीत जापान में 4 दिनो बाद कोई अभी तक जिंदा होने की आशाएँ लुप्त हो रही थी।  <eos>',\n",
       " '<sos> वर्ग का पूर्णा क्या था?  <eos>',\n",
       " '<sos> मैं अपना काम कर चुका हूँ।  <eos>',\n",
       " '<sos> राष्ट्रीय मनः स्वास्थ्य कार्यक्रम <eos>',\n",
       " '<sos> क्रियावली <eos>',\n",
       " '<sos> मुद्रास्फीति की दर प्रत्यक्ष रूप से उपरली डिग्री में है।  <eos>',\n",
       " '<sos> वाहिका में रूधिर अवपंक रक्त प्रवाह में अवरोध का कारण है <eos>',\n",
       " '<sos> उपकरण <eos>',\n",
       " '<sos> हरकत-उल-जिहाद-ए-इस्लामी <eos>',\n",
       " '<sos> URL … <eos>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a79312-9f66-4dad-8ee2-4221cb281e51",
   "metadata": {},
   "source": [
    "## Tokenization (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a707677-b088-4330-8608-39203d194344",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer(filters='0123456789!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~', oov_token=\"<unk>\")\n",
    "tgt_tokenizer = Tokenizer(filters='0123456789!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~', oov_token=\"<unk>\")\n",
    "\n",
    "# Force <pad> to exist\n",
    "src_tokenizer.fit_on_texts([\"<pad>\"] + src_texts)\n",
    "tgt_tokenizer.fit_on_texts([\"<pad>\"] + tgt_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43e61473-59f9-4228-a166-138ee90888ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.legacy.preprocessing.text.Tokenizer at 0x17eae5dda90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6611757b-b6b4-44a7-ad89-291740700196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5022"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokenizer.word_index[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a308a2-3b2d-411f-b488-8067cc849b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4638"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer.word_index[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f745a1e-4d76-4f80-a26c-4806aab8a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokenizer.word_index[\"<pad>\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b47784-4b67-455f-9473-a9658f2878be",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer.word_index[\"<pad>\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfa29dff-dc31-46c6-bfbe-2b4076c59765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokenizer.word_index[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0937f47a-a444-4b74-a4b7-55f58fb8ac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer.word_index[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59673a2b-0da1-4b3d-b601-128a51b1b73e",
   "metadata": {},
   "source": [
    "## Convert to Sequences + Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f32a906-106d-4007-a3fe-a140f284e4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SRC_LEN = max(len(seq) for seq in tgt_texts)\n",
    "MAX_TGT_LEN = max(len(seq) for seq in tgt_texts)\n",
    "\n",
    "src_seq = src_tokenizer.texts_to_sequences(src_texts)\n",
    "tgt_seq = tgt_tokenizer.texts_to_sequences(tgt_texts)\n",
    "\n",
    "src_padded = pad_sequences(src_seq, maxlen=MAX_SRC_LEN, padding=\"post\")\n",
    "tgt_padded = pad_sequences(tgt_seq, maxlen=MAX_TGT_LEN, padding=\"post\")\n",
    "tgt_tokenizer.word_index[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad9cf6d-7a35-47b0-9d85-5e5252d8a173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  2356,\n",
       "  2357,\n",
       "  14,\n",
       "  5023,\n",
       "  9,\n",
       "  5024,\n",
       "  1844,\n",
       "  6,\n",
       "  5025,\n",
       "  78,\n",
       "  38,\n",
       "  307,\n",
       "  55,\n",
       "  5026,\n",
       "  94,\n",
       "  8,\n",
       "  5027,\n",
       "  5028,\n",
       "  22,\n",
       "  178,\n",
       "  208,\n",
       "  3],\n",
       " [2, 665, 11, 5029, 45, 47, 3],\n",
       " [2, 70, 131, 111, 28, 439, 564, 3],\n",
       " [2, 156, 5030, 412, 375, 3],\n",
       " [2, 5031, 3]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e21b11-c49d-4f8c-892a-490254eb1b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  317,\n",
       "  72,\n",
       "  4,\n",
       "  4639,\n",
       "  4640,\n",
       "  4641,\n",
       "  1709,\n",
       "  2196,\n",
       "  5,\n",
       "  1710,\n",
       "  849,\n",
       "  191,\n",
       "  1404,\n",
       "  56,\n",
       "  4642,\n",
       "  3],\n",
       " [2, 52, 31, 4643, 4, 1160, 3],\n",
       " [2, 39, 23, 288, 399, 87, 152, 3],\n",
       " [2, 120, 1711, 400, 466, 3],\n",
       " [2, 1405, 3]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28bd89f3-8b2e-4a90-b520-57464714dac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,  317,   72, ...,    0,    0,    0],\n",
       "       [   2,   52,   31, ...,    0,    0,    0],\n",
       "       [   2,   39,   23, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2, 1161,    3, ...,    0,    0,    0],\n",
       "       [   2, 4647, 3014, ...,    0,    0,    0],\n",
       "       [   2,  755, 3016, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_padded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f82b671-6378-4304-ab3a-017bdca1106f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 2356, 2357, ...,    0,    0,    0],\n",
       "       [   2,  665,   11, ...,    0,    0,    0],\n",
       "       [   2,   70,  131, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2, 1129,    3, ...,    0,    0,    0],\n",
       "       [   2, 5037, 2360, ...,    0,    0,    0],\n",
       "       [   2, 3205, 2361, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_padded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84dc5a-9a84-45e3-8891-a2a7021ae7d2",
   "metadata": {},
   "source": [
    "## Vocabulary Sizes + Special Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "826b3703-2a43-4057-a182-d4e4d021172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\n",
    "TGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\n",
    "\n",
    "PAD_IDX = tgt_tokenizer.word_index[\"<pad>\"]\n",
    "SOS_IDX = tgt_tokenizer.word_index[\"<sos>\"]\n",
    "EOS_IDX = tgt_tokenizer.word_index[\"<eos>\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "354bece9-e97c-4e43-9c23-fea850b758f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21684e19-43cd-4f78-905c-04be7375679a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tgt_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bd88e-baba-4b4f-902b-b3132e623709",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93a9cb84-a94b-4a7c-bfc3-5ffb1976b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = torch.tensor(src, dtype=torch.long)\n",
    "        self.tgt = torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81526b8c-3535-4e78-9f0b-4fcb4b36dcf0",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d5d81d-d2a2-439a-998d-954eac367b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(src_padded, tgt_padded)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657097c-9392-4a0f-a03d-5caa6c6e40ab",
   "metadata": {},
   "source": [
    "## Encoder Decoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b520750-e805-4ca1-a796-8cf485a04b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [B, src_len]\n",
    "        embedded = self.embedding(src)           # [B, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs: [B, src_len, H]\n",
    "        return outputs, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f3ee2c3-0187-4cb2-b380-e18c6b002420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: [B, H]\n",
    "        # encoder_outputs: [B, src_len, H]\n",
    "\n",
    "        scores = torch.bmm(\n",
    "            encoder_outputs,\n",
    "            decoder_hidden.unsqueeze(2)\n",
    "        ).squeeze(2)              # [B, src_len]\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # [B, src_len]\n",
    "\n",
    "        context = torch.bmm(\n",
    "            attn_weights.unsqueeze(1),\n",
    "            encoder_outputs\n",
    "        ).squeeze(1)              # [B, H]\n",
    "\n",
    "        return context, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14bab5aa-e7fe-4b0f-ac71-e087a5b4dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = LuongAttention()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim + hidden_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
    "        # input_token: [B]\n",
    "        input_token = input_token.unsqueeze(1)     # [B, 1]\n",
    "\n",
    "        embedded = self.embedding(input_token)     # [B, 1, emb_dim]\n",
    "\n",
    "        context, attn = self.attention(\n",
    "            hidden[-1], encoder_outputs\n",
    "        )                                           # [B, H]\n",
    "\n",
    "        context = context.unsqueeze(1)              # [B, 1, H]\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(\n",
    "            lstm_input, (hidden, cell)\n",
    "        )\n",
    "\n",
    "        output = output.squeeze(1)                  # [B, H]\n",
    "        context = context.squeeze(1)                # [B, H]\n",
    "\n",
    "        logits = self.fc(torch.cat((output, context), dim=1))\n",
    "\n",
    "        return logits, hidden, cell, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2c6dd4c-27d1-47fb-85cd-5eb8f2b02142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            batch_size, tgt_len - 1, vocab_size\n",
    "        ).to(src.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input_token = tgt[:, 0]   # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, _ = self.decoder(\n",
    "                input_token, hidden, cell, encoder_outputs\n",
    "            )\n",
    "            outputs[:, t - 1] = output\n",
    "            input_token = tgt[:, t]   # teacher forcing\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65beb2af-c06c-4828-b3a0-e0f16f2fd5fc",
   "metadata": {},
   "source": [
    "## model initialzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f416b7a9-3eee-455e-accd-f40b2e0a4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c9171aa-e496-48fd-b665-6dc61e862f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(SRC_VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e1ae9d4-baf9-4f30-af54-8608c73b2c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(11062, 100)\n",
       "    (lstm): LSTM(100, 100, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(13221, 100)\n",
       "    (attention): LuongAttention()\n",
       "    (lstm): LSTM(200, 100, batch_first=True)\n",
       "    (fc): Linear(in_features=200, out_features=13221, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac542dc-f88a-4eb1-baf4-f0afabd89a58",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f07676d5-486c-4efd-8b6c-014bdc97c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47b07ffb-2208-4599-b966-53de48940798",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60515087-a84e-4710-8570-9a28f3d81c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 320\n",
      "200 / 320\n",
      "300 / 320\n",
      "Epoch 1/1 | Loss: 2280.3328\n",
      "CPU times: total: 23min 35s\n",
      "Wall time: 1h 7min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    i=0\n",
    "\n",
    "    for src_batch, tgt_batch in loader:\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(src_batch, tgt_batch)\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[-1]),\n",
    "            tgt_batch[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        i+=1\n",
    "        #print(i)\n",
    "        if i % 100 == 0:\n",
    "            print(i,'/',320)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adf5ec0f-dcf3-48d4-91f8-1cd44dffc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34723a8e-daae-456c-8c48-0007007ba727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 320\n",
      "200 / 320\n",
      "300 / 320\n",
      "Epoch 1/1 | Loss: 2125.0483\n",
      "CPU times: total: 23min\n",
      "Wall time: 1h 7min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    i=0\n",
    "\n",
    "    for src_batch, tgt_batch in loader:\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(src_batch, tgt_batch)\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[-1]),\n",
    "            tgt_batch[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        i+=1\n",
    "        #print(i)\n",
    "        if i % 100 == 0:\n",
    "            print(i,'/',320)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c01171-7a5e-4874-8ebc-e3ff50ae5456",
   "metadata": {},
   "source": [
    "## pridiction functiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d5c1063-0628-4522-8162-7fdc2d0a4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, max_len=100):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seq = src_tokenizer.texts_to_sequences(\n",
    "            [f\"<sos> {sentence.lower()} <eos>\"]\n",
    "        )\n",
    "        seq = pad_sequences(seq, maxlen=MAX_SRC_LEN, padding=\"post\")\n",
    "        src = torch.tensor(seq, dtype=torch.long).to(device)\n",
    "\n",
    "        #  ATTENTION CHANGE HERE\n",
    "        encoder_outputs, hidden, cell = encoder(src)\n",
    "\n",
    "        input_token = torch.tensor([SOS_IDX], dtype=torch.long).to(device)\n",
    "        result = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            #  ATTENTION CHANGE HERE\n",
    "            output, hidden, cell, _ = decoder(\n",
    "                input_token, hidden, cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "            token = output.argmax(dim=1).item()\n",
    "\n",
    "            # ---- STOP condition ----\n",
    "            if token == EOS_IDX:\n",
    "                #break\n",
    "                pass\n",
    "\n",
    "            # ---- SKIP PAD safely ----\n",
    "            if token == PAD_IDX:\n",
    "                input_token = torch.tensor([SOS_IDX], dtype=torch.long).to(device)\n",
    "                continue\n",
    "\n",
    "            result.append(tgt_tokenizer.index_word[token])\n",
    "            input_token = torch.tensor([token], dtype=torch.long).to(device)\n",
    "\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6bdca459-d17c-486c-9fa4-af179e2b7fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'यह <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> है <eos> <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos> <eos> है <eos>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"i have already done my work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28835d56-2dab-4080-b128-56327c48ff78",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62de7211-0cd3-42df-85a3-eee6606e25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root=Path(\"model\")\n",
    "root.mkdir(exist_ok=True)\n",
    "path= root / \"eng_to_hindi_Attention.pth\"\n",
    "torch.save(model,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45dc5b20-e160-4b82-869f-c602b4f8b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "tokenizer_json = src_tokenizer.to_json()\n",
    "\n",
    "with open(\"model/src_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tokenizer_json)\n",
    "\n",
    "tgt_tokenizer_json = tgt_tokenizer.to_json()\n",
    "\n",
    "with open(\"model/tgt_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tgt_tokenizer_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f744ce58-a3d9-448c-b645-cbfc143d0a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SRC_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccc1b5c2-ca2f-417c-98fa-c841e731b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TGT_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b59ed-4e1d-4d8c-8f7e-5479ba2965b8",
   "metadata": {},
   "source": [
    "# Development : more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ba28c7b-4fb5-4601-9003-48e80b670eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21e4b005-ad10-4624-830a-c9a145d2e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "caf2d1e0-9738-430c-afb5-92547f5350b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SRC_LEN=1063\n",
    "#by first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9c45631-0a15-44f5-9163-5210c1f331ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TGT_LEN =1063\n",
    "#by first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a186f96-1493-42c8-92d0-fdc6b57be1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "\n",
    "with open(\"model/src_tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    src_tokenizer = tokenizer_from_json(f.read())\n",
    "\n",
    "with open(\"model/tgt_tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tgt_tokenizer = tokenizer_from_json(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a5acf6b-9323-457c-9a5f-10943e7b1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\n",
    "TGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\n",
    "\n",
    "PAD_IDX = tgt_tokenizer.word_index[\"<pad>\"]\n",
    "SOS_IDX = tgt_tokenizer.word_index[\"<sos>\"]\n",
    "EOS_IDX = tgt_tokenizer.word_index[\"<eos>\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102fa90-977f-43ea-834f-6d48168f3373",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87e39d46-b14c-4ae0-a0de-a0f9a18e2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fc6c8ad-33f5-4382-9761-fde04b742094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [B, src_len]\n",
    "        embedded = self.embedding(src)           # [B, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs: [B, src_len, H]\n",
    "        return outputs, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57b2ccc8-d86f-4844-a58f-df6b5049eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: [B, H]\n",
    "        # encoder_outputs: [B, src_len, H]\n",
    "\n",
    "        scores = torch.bmm(\n",
    "            encoder_outputs,\n",
    "            decoder_hidden.unsqueeze(2)\n",
    "        ).squeeze(2)              # [B, src_len]\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # [B, src_len]\n",
    "\n",
    "        context = torch.bmm(\n",
    "            attn_weights.unsqueeze(1),\n",
    "            encoder_outputs\n",
    "        ).squeeze(1)              # [B, H]\n",
    "\n",
    "        return context, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "71debc34-7bb9-4efa-8f69-100b445c1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = LuongAttention()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim + hidden_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
    "        # input_token: [B]\n",
    "        input_token = input_token.unsqueeze(1)     # [B, 1]\n",
    "\n",
    "        embedded = self.embedding(input_token)     # [B, 1, emb_dim]\n",
    "\n",
    "        context, attn = self.attention(\n",
    "            hidden[-1], encoder_outputs\n",
    "        )                                           # [B, H]\n",
    "\n",
    "        context = context.unsqueeze(1)              # [B, 1, H]\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(\n",
    "            lstm_input, (hidden, cell)\n",
    "        )\n",
    "\n",
    "        output = output.squeeze(1)                  # [B, H]\n",
    "        context = context.squeeze(1)                # [B, H]\n",
    "\n",
    "        logits = self.fc(torch.cat((output, context), dim=1))\n",
    "\n",
    "        return logits, hidden, cell, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "41db2497-ce05-4c7f-a950-439b200435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d43cfa72-0b16-473c-b36b-0f8a3b35e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(SRC_VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95b1eeea-df65-4e2c-92b2-bdc42e4253fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            batch_size, tgt_len - 1, vocab_size\n",
    "        ).to(src.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input_token = tgt[:, 0]   # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, _ = self.decoder(\n",
    "                input_token, hidden, cell, encoder_outputs\n",
    "            )\n",
    "            outputs[:, t - 1] = output\n",
    "            input_token = tgt[:, t]   # teacher forcing\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b183ca7d-5045-4aee-8d37-693592126531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(11062, 100)\n",
       "    (lstm): LSTM(100, 100, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(13221, 100)\n",
       "    (attention): LuongAttention()\n",
       "    (lstm): LSTM(200, 100, batch_first=True)\n",
       "    (fc): Linear(in_features=200, out_features=13221, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "model_dir = Path(\"model\")\n",
    "load_path = model_dir / \"eng_to_hindi_Attention.pth\"\n",
    "\n",
    "# tell torch to allow full unpickling (risky if file is untrusted)\n",
    "model = torch.load(load_path, weights_only=False)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405b5c2-6cdb-416e-af49-1498bdc970e1",
   "metadata": {},
   "source": [
    "## pridiction functiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee62c7ea-57a7-4a3f-acca-0e402b00deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, max_len=100):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seq = src_tokenizer.texts_to_sequences(\n",
    "            [f\"<sos> {sentence.lower()} <eos>\"]\n",
    "        )\n",
    "        seq = pad_sequences(seq, maxlen=MAX_SRC_LEN, padding=\"post\")\n",
    "        src = torch.tensor(seq, dtype=torch.long).to(device)\n",
    "\n",
    "        #  ATTENTION CHANGE HERE\n",
    "        encoder_outputs, hidden, cell = encoder(src)\n",
    "\n",
    "        input_token = torch.tensor([SOS_IDX], dtype=torch.long).to(device)\n",
    "        result = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            #  ATTENTION CHANGE HERE\n",
    "            output, hidden, cell, _ = decoder(\n",
    "                input_token, hidden, cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "            token = output.argmax(dim=1).item()\n",
    "\n",
    "            # ---- STOP condition ----\n",
    "            if token == EOS_IDX:\n",
    "                #break\n",
    "                pass\n",
    "\n",
    "            # ---- SKIP PAD safely ----\n",
    "            if token == PAD_IDX:\n",
    "                input_token = torch.tensor([SOS_IDX], dtype=torch.long).to(device)\n",
    "                continue\n",
    "\n",
    "            result.append(tgt_tokenizer.index_word[token])\n",
    "            input_token = torch.tensor([token], dtype=torch.long).to(device)\n",
    "\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "760dda6c-e585-4230-b00c-d2b4f5da521f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'पुरावस्तु सहज विशिष्टाओं प्रवेश्य फ़ैसले सहज श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों प्रतिरूपण श्वेतपत्र गुट खिलॊने खिलॊने श्वेतपत्र खिलॊने खिलॊने हृदयों'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"the inflation rate is apparently in the ascending degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5faac-e2f7-4a9a-a288-70708abba155",
   "metadata": {},
   "source": [
    "## Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25832757-a7ee-40ff-85d3-d177a6e957a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "746eb845-d4c3-479c-a8bd-75735f13fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "60385f8f-d5f3-4f2f-b069-ee3b320212d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:19\u001b[0m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    649\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[0;32m    355\u001b[0m     tensors,\n\u001b[0;32m    356\u001b[0m     grad_tensors_,\n\u001b[0;32m    357\u001b[0m     retain_graph,\n\u001b[0;32m    358\u001b[0m     create_graph,\n\u001b[0;32m    359\u001b[0m     inputs_tuple,\n\u001b[0;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    362\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    i=0\n",
    "\n",
    "    for src_batch, tgt_batch in loader:\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(src_batch, tgt_batch)\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[-1]),\n",
    "            tgt_batch[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        i+=1\n",
    "        if i % 20 == 0:\n",
    "            print(i,'/',320)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7b1dd-2c5d-4eee-a2d9-de8817f7b6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdcf8c-956d-4ff2-b7cb-29c24a26ba63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c049c8d-61e3-44b7-a496-07e48c210fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac56b4e-ce25-4f6b-b898-30657cf26379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
